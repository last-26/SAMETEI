#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen OCR API Server
S√ºrekli √ßalƒ±≈üan model servisi - istemciler API √ºzerinden baƒülanƒ±r
"""

import os
import io
import base64
import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch
import uvicorn

# Global deƒüi≈ükenler
model = None
processor = None
device = None
model_loaded = False

# Logging ayarlarƒ±
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Request/Response modelleri
class OCRRequest(BaseModel):
    image: str  # Base64 encoded image
    prompt: str = "Bu g√∂r√ºnt√ºdeki T√úM metni √ßƒ±kar. T√ºrk√ße karakterleri koru (ƒü, √º, ≈ü, ƒ±, ƒ∞, √∂, √ß)."
    max_tokens: int = 1024

class OCRResponse(BaseModel):
    success: bool
    text: str = ""
    error: str = ""
    processing_time: float = 0.0

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Uygulama ba≈ülatma ve kapatma lifecycle"""
    # Ba≈ülatma
    logger.info("üöÄ Qwen OCR API ba≈ülatƒ±lƒ±yor...")
    await load_model_async()

    yield

    # Kapatma
    logger.info("‚èπÔ∏è Qwen OCR API kapatƒ±lƒ±yor...")
    await cleanup_model()

async def load_model_async():
    """Qwen modelini asenkron y√ºkle"""
    global model, processor, device, model_loaded

    try:
        logger.info("ü§ñ Qwen modeli y√ºkleniyor...")

        # GPU kontrol√º
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"üìä Kullanƒ±lacak cihaz: {device}")

        # Model ID - Hugging Face'den y√ºkle
        model_id = "Qwen/Qwen2.5-VL-3B-Instruct"

        # GPU optimizasyonlarƒ±
        if torch.cuda.is_available():
            # Memory fraction ayarƒ±
            torch.cuda.set_per_process_memory_fraction(0.85)
            # Diƒüer optimizasyonlar
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = "max_split_size_mb:256,garbage_collection_threshold:0.6,expandable_segments:True"

        # Processor y√ºkle
        processor = AutoProcessor.from_pretrained(
            model_id,
            trust_remote_code=True,
            min_pixels=640 * 28 * 28,
            max_pixels=1024 * 28 * 28,
        )

        # Model y√ºkle
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else "cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            max_memory={0: "5.1GB", "cpu": "8GB"} if torch.cuda.is_available() else None,
        )

        model.eval()
        model_loaded = True
        logger.info("‚úÖ Model ba≈üarƒ±yla y√ºklendi ve hazƒ±r!")

    except Exception as e:
        logger.error(f"‚ùå Model y√ºkleme hatasƒ±: {e}")
        model_loaded = False
        raise

async def cleanup_model():
    """Model temizliƒüi"""
    global model, processor, device, model_loaded

    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        model = None
        processor = None
        device = None
        model_loaded = False

        logger.info("üßπ Model temizliƒüi tamamlandƒ±")

    except Exception as e:
        logger.warning(f"Model temizliƒüi hatasƒ±: {e}")

# FastAPI uygulamasƒ±
app = FastAPI(
    title="Qwen OCR API",
    description="Qwen2.5-VL ile g√∂r√ºnt√ºden metin √ßƒ±karma servisi",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ayarlarƒ±
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """API durumu"""
    return {
        "status": "running",
        "model_loaded": model_loaded,
        "device": str(device) if device else "not loaded",
        "model": "Qwen/Qwen2.5-VL-3B-Instruct"
    }

@app.get("/health")
async def health_check():
    """Saƒülƒ±k kontrol√º"""
    return {
        "status": "healthy" if model_loaded else "model_not_loaded",
        "model_loaded": model_loaded,
        "gpu_memory": torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0,
        "gpu_used": torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0
    }

@app.post("/ocr", response_model=OCRResponse)
async def extract_text(request: OCRRequest, background_tasks: BackgroundTasks):
    """G√∂r√ºnt√ºden metin √ßƒ±karma"""

    if not model_loaded:
        raise HTTPException(status_code=503, detail="Model hen√ºz y√ºklenmedi")

    import time
    start_time = time.time()

    try:
        logger.info("üîç OCR isteƒüi i≈üleniyor...")

        # Base64'ten g√∂r√ºnt√ºy√º decode et
        image_data = base64.b64decode(request.image)
        image = Image.open(io.BytesIO(image_data))

        # Basit preprocessing
        if image.mode != 'L':
            image = image.convert('L')
            logger.info("Grayscale uygulandƒ±")

        # Prompt hazƒ±rla
        prompt = request.prompt

        # Mesajlarƒ± hazƒ±rla
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image", "image": image},
                ],
            }
        ]

        # Model √ßƒ±karƒ±mƒ±
        prompt_text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        image_inputs, video_inputs = process_vision_info(messages)

        inputs = processor(
            text=[prompt_text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(device)

        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=0.0,
                do_sample=False,
                num_beams=1,
                eos_token_id=getattr(processor.tokenizer, 'eos_token_id', None),
                pad_token_id=getattr(processor.tokenizer, 'pad_token_id', None),
            )

        # √áƒ±ktƒ±yƒ± i≈üle
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]

        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]

        # Temizle
        clean_text = clean_output_text(output_text)
        processing_time = time.time() - start_time

        logger.info("%.2f", processing_time)
        return OCRResponse(
            success=True,
            text=clean_text,
            processing_time=processing_time
        )

    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"‚ùå OCR hatasƒ±: {e}")

        return OCRResponse(
            success=False,
            error=str(e),
            processing_time=processing_time
        )

def clean_output_text(text):
    """√áƒ±ktƒ± metnini temizleme"""
    if not text:
        return ""

    import re

    # Gereksiz ba≈ülangƒ±√ß metinlerini temizle
    text = re.sub(r"^Here is the extracted.*?:\s*", "", text, flags=re.IGNORECASE)
    text = re.sub(r"^Extracted text:\s*", "", text, flags=re.IGNORECASE)
    text = re.sub(r"^The extracted.*?:\s*", "", text, flags=re.IGNORECASE)

    # Code block'lardan √ßƒ±kar
    fence = re.compile(r"```[a-zA-Z0-9]*\n([\s\S]*?)\n```")
    match = fence.search(text)
    if match:
        text = match.group(1).strip()

    # Fazla bo≈üluklarƒ± temizle
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
